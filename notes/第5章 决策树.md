# 第 5 章 决 策 树

决策树（decision tree）是一种基本的分类与回归方法，决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可以认为是**if-then**的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其**主要优点是模型具有可读性，分类速度快**。决策树学习通常包括3个步骤：**特征选择、决策树的生成和决策树的修剪**。

## 5.1 决策树模型与学习

### 5.1.1 决策树模型

#### 定义 5.1 （决策树）

**分类决策树模型是一种描述对实例进行分类的树形结构**，决策树**由结点（node）和有向边（directed edge）组成**。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性，

用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配给其子结点；每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点，将实例分配给叶结点的类中。

![image-20220419212717117](http://qiniu.lianghao.work/markdown/image-20220419212717117.png)

### 5.1.2 决策树与IF-THEN规则

可以将决策树看成一个*if-then*规则的集合。将决策树转换成*if-then*规则的过程：由决策树的根节点到叶结点的每一条路径构建一条规则：**路径上内部结点的特征对应着规则的条件**， **叶结点的类对应着规则的结论**。决策树的路径或其对应着if-then规则集合有一个重要的性质：**互斥并且完备**。即每一个实例都被一条路径或一个规则所**覆盖**，而且只被一条路径或一条规则所**覆盖**。这里所谓的**覆盖**是指**实例的特征与路径上的特征一致或实例满足规则的条件**。

### 5.1.3 决策树与条件概率分布

**决策树还表示给定特征条件下类的条件概率分布**，这一条件概率分布定义在特征空间的一个划分（partition）上。*将特征空间划分为互补相交的单元（cell）或区域（region），并在每个单元定义一个类的概率分布就构成了一个条件概率分布*。决策树的一条路径对应于划分的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类条件概率分布组成。假设$X$为**表示特征**的随机变量，$Y$为**表示类**的随机变量，那么这个条件概率分布可以表示为$P(Y|X)$。$X$取值于给定划分（partition）下单元的集合，$Y$取值于类的集合。各叶结点（单元）上的条件概率往往偏向某一个类，即属于某一类的概率比较大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。

![image-20220419220227050](http://qiniu.lianghao.work/markdown/image-20220419220227050.png)

### 5.1.4 决策树学习

假设给定训练数据集
$$
D=\{(x_1, y_1),(x_2, y_2),\cdots, (x_N,y_N)\}
$$
其中，$x_i= (x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$为输入特例（特征向量），$n$为特征个数，$y_i\in\{1, 2, 3,\cdots, K\}$为类标记，$i=1, 2, \cdots, N$，$N$为特征容量。决策树学习的**目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类**。

**决策树学习的本质**是从训练数据集中**归纳一组分类规则**，选择**一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力**。

**决策树学习用损失函数表示**这一目标，决策树学习的损失函数通产是**正则化的极大似然函数**，决策树学习的**策略**是以损失函数为目标函数的**最小化**。

**决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程**。这一过程**对应着特征空间的划分**，也**对应着决策树的构建**。

开始，构建根结点，将所有训练数据都放在根结点，选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中。如果不能被基本正确分类，那么就对这些子集选择我新的最优特征，并对其进行分割，构建相应的结点。如此递归下去，直至所有训练数据子集被基本正确分类或者没有合适的特征为止。最后每个子集被分到叶结点上，即有了明确的类，就生成了一颗决策树。

**决策树学习常用的算法有ID3、C4.5和CART**。

## 5.2 特征选择

### 5.2.1 特征选择问题

**特征选择在于选取对训练数据具有分类能力的特征**，这样可以提高决策树学习的效率。

**特征选择的准则是信息增益（information gain）或信息增益比**。

**熵（entropy）是表示随机变量不确定性的度量**。设$X$是一个取有限个值的离散随机变量，其概率分布为：
$$
P(X=x_i)=p_i, i=1,2,\cdots,n
$$
那么随机变量$X$的熵定义为：
$$
H(X)=-\sum_{i=1}^np_i\log p_i
$$
**熵只依赖于$X$的分布，而与$X$的取值无关，熵的单位可分为比特（bit，对数以2为底）和纳特（nat，对数以e为底）**。熵越大，随机变量的不确定性越大，$0\le H(p)\le\log n$。

当随机变量$X$服从伯努利（二项）分布时，熵$H(P)$随概率$P(X=0)=p$的变化如下

![image-20220503192151303](http://qiniu.lianghao.work/markdown/image-20220503192151303.png)

**条件熵（conditional entropy）$H(Y|X)$**表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。条件熵被定义为**X给定条件下Y的条件概率分布的熵对X的数学期望**
$$
H(Y|X)=\sum_{i=1}^nH(Y|X=x_i)*P(X=x_i),\ i=1, 2,\cdots,n
$$

$$
\begin{aligned}
H(Y|X=x_1) &= -\sum_{i=1}^n p(y_i|x_1)\log p(y_i|x_1)\\
H(Y|X=x_2) &= -\sum_{i=1}^n p(y_i|x_2)\log p(y_i|x_2)\\
&\vdots\\
H(Y|X=x_n) &= -\sum_{i=1}^n p(y_i|x_n)\log p(y_i|x_n)\\
\therefore H(Y|X) &= H(Y|X=x_1)*p(X=x_1)+\cdots+H(Y|X=x_n)*p(X=x_n)\\
&=\sum_{i=1}^n p(x_i)\cdot H(Y|x_i)

\end{aligned}
$$

当**熵和条件熵**中的概率由数据估计（极大似然估计）得到时，所对应的熵与条件熵分别称为**经验熵 (empirical entropy)**和**经验条件熵(empirical conditional entropy)**。

**信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度**。

### 定义 5.2（信息增益）

**特征$A$对训练数据集$D$的信息增益$g(D, A)$，定义为集合$D$的经验熵$H(D)$与特征A给定条件下的$D$的经验条件熵$H(D|A)$之差**，即
$$
g(D,A)=H(D)-H(D|A)
$$
一般来说， **熵$H(Y)$与条件熵$H(Y|X)$**之差称为**互信息(mutual information)**，**决策树学习中的信息增益等价于训练数据中类与特征的互信息**。

如果数据集D表示类别，$A$表示特征。对于数据集D来说，信息增益依赖于特征。有了特征A，不确定性会降低，信息才有所增益。**不同特征对于同一数据集往往具有不同的信息增益，信息增益大的特征具有更强的分类能力。**

根据**信息增益准则**的特征选择方法：对训练数据集（或子集）D，计算每个特征的信息增益，并比较每个特征的信息增益，并比较他们的大小，选择信息增益最大特征。

*假设训练集为D，$|D|$表示样本容量（样本个数），由$K$个类$C_K$，$|C_K|$表示类属于$C_K$的样本个数，$\sum_{i=1}^K|C_i|=|D|$。假设特征$A$有$n$个不同的取值$\{a_1,a_2,\cdots,a_n\}$，根据特征$A$的取值将数据集D划分为n个子集，$\sum_{i=1}^n |D_i|=|D|$，记子集$D_i$中类属于$C_K$的样本的集合为$D_ik$，即$D_ik=D_i\cap C_K$*，信息增益的算法如下：

### 算法 5.1 （信息增益的算法）

**输入：训练数据集$D$和特征$A$**

**输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$**

（1） 计算数据集$D$的经验熵（empirical entropy）$H(D)$
$$
H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2 \frac{|C_k|}{|D|}
$$
（2）计算特征A对数据集$D$的经验条件熵$H(D|A)$
$$
\begin{aligned}
H(D|A) &=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)\\
&=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_ik|}{|D_i|}\log_2\frac{|D_ik|}{|D_i|}
\end{aligned}
$$
（3）计算信息增益（information gain）
$$
g(D, A)=H(D)-H(D|A)
$$

### 5.2.3 信息增益比

以*信息增益*作为划分训练数据集的特征，存在**偏向于选择取值较多的特征**的问题。使用**信息增益比(information gain ratio)**可以对这一问题进行校正。这是特征选择的另一准则。

###  定义 5.3 （信息增益比）

**特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D关于特征$A$的值的熵$H_A(D)$之比**
$$
g_R(D, A)=\frac{g(D, A)}{H_A(D)}\\
H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}
$$


##  5.3 决策树的生成

### 5.3.1 ID3 算法

ID3算法的核心是**在决策树各个结点上应用信息增益准则选择特征，从而递归地构建决策树**。具体方法是：从根节点（root node）开始，对结点计算所有可能的特征的信息增益，**选择信息增益最大的特征作为结点的特征**，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。**ID3相当于用极大似然法进行概率模型的选择**。

### 算法 5.2 （ID3 算法）

输入：训练数据集$D$，特征集$A$阈值$\epsilon$

输出：决策树$T$

（1）若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将类$C_K$作为该结点的类标记，返回$T$

（2）若$A=\empty$，则$T$为单结点树，并将$D$中**实例数最大**的类$C_k$作为该结点的类标记，返回$T$；否则，按照**信息增益算法**计算$A$中各特征$A_i$对$D$的信息增益，选择信息增益最大的特征$A_g$

（3）如果$A_g$的信息增益小于阈值$\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；否则，对$A_g$的每一个可能取值$a_i$，依照$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点后成树$T$，返回$T$

（4）对于第$i$个子结点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归地调用**步骤(1)~(3)**，得到子树$T_i$，返回$T_i$

### 5.3.2 C4.5 生成算法

C4.5算法在ID3算法上进行了改进：**使用信息增益比代替信息增益来选择当前最优特征**

## 5.4 决策树的剪枝

决策树生成算法递归地产生决策树，直到不能继续下去为止，这样产生的树对训练数据的分类很准确，但对为止的测试数据的分类往往不准确，即出现**过拟合现象**。

在决策树学习中将已生成的树进行简化的过程称为**剪枝 pruning**。

决策树的剪枝往往通过**极小化决策树整体的损失函数或代价函数**来实现。设树$T$的叶结点个数为$|T|$，$t$是$T$的叶结点，该叶结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk},\ k=1,2, 3,\cdots, K$个，$H_t(T)$为叶结点$t$上的经验熵，$\alpha\ge 0$为参数，则决策树学习的损失函数为
$$
\begin{aligned}
C_\alpha(T) &= \sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|\\
H_t(T) &=-\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}\\
C(T) &= -\sum_{t=1}^{|T|}\sum_k^KN_{tk}\log\frac{N_{tk}}{N_t}\\
C_\alpha(T)&=C(T)+\alpha|T|
\end{aligned}
$$

