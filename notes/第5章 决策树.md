# 第 5 章 决 策 树

决策树（decision tree）是一种基本的分类与回归方法，决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可以认为是**if-then**的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其**主要优点是模型具有可读性，分类速度快**。决策树学习通常包括3个步骤：**特征选择、决策树的生成和决策树的修剪**。

## 5.1 决策树模型与学习

### 5.1.1 决策树模型

#### 定义 5.1 （决策树）

**分类决策树模型是一种描述对实例进行分类的树形结构**，决策树**由结点（node）和有向边（directed edge）组成**。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性，

用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配给其子结点；每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点，将实例分配给叶结点的类中。

![image-20220419212717117](http://qiniu.lianghao.work/markdown/image-20220419212717117.png)

### 5.1.2 决策树与IF-THEN规则

可以将决策树看成一个*if-then*规则的集合。将决策树转换成*if-then*规则的过程：由决策树的根节点到叶结点的每一条路径构建一条规则：**路径上内部结点的特征对应着规则的条件**， **叶结点的类对应着规则的结论**。决策树的路径或其对应着if-then规则集合有一个重要的性质：**互斥并且完备**。即每一个实例都被一条路径或一个规则所**覆盖**，而且只被一条路径或一条规则所**覆盖**。这里所谓的**覆盖**是指**实例的特征与路径上的特征一致或实例满足规则的条件**。

### 5.1.3 决策树与条件概率分布

**决策树还表示给定特征条件下类的条件概率分布**，这一条件概率分布定义在特征空间的一个划分（partition）上。*将特征空间划分为互补相交的单元（cell）或区域（region），并在每个单元定义一个类的概率分布就构成了一个条件概率分布*。决策树的一条路径对应于划分的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类条件概率分布组成。假设$X$为**表示特征**的随机变量，$Y$为**表示类**的随机变量，那么这个条件概率分布可以表示为$P(Y|X)$。$X$取值于给定划分（partition）下单元的集合，$Y$取值于类的集合。各叶结点（单元）上的条件概率往往偏向某一个类，即属于某一类的概率比较大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。

![image-20220419220227050](http://qiniu.lianghao.work/markdown/image-20220419220227050.png)

### 5.1.4 决策树学习

假设给定训练数据集
$$
D=\{(x_1, y_1),(x_2, y_2),\cdots, (x_N,y_N)\}
$$
其中，$x_i= (x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$为输入特例（特征向量），$n$为特征个数，$y_i\in\{1, 2, 3,\cdots, K\}$为类标记，$i=1, 2, \cdots, N$，$N$为特征容量。决策树学习的**目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类**。

**决策树学习的本质**是从训练数据集中**归纳一组分类规则**，选择**一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力**。

**决策树学习用损失函数表示**这一目标，决策树学习的损失函数通产是**正则化的极大似然函数**，决策树学习的**策略**是以损失函数为目标函数的**最小化**。

**决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程**。这一过程**对应着特征空间的划分**，也**对应着决策树的构建**。

开始，构建根结点，将所有训练数据都放在根结点，选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中。如果不能被基本正确分类，那么就对这些子集选择我新的最优特征，并对其进行分割，构建相应的结点。如此递归下去，直至所有训练数据子集被基本正确分类或者没有合适的特征为止。最后每个子集被分到叶结点上，即有了明确的类，就生成了一颗决策树。

**决策树学习常用的算法有ID3、C4.5和CART**。

## 5.2 特征选择

### 5.2.1 特征选择问题

**特征选择在于选取对训练数据具有分类能力的特征**，这样可以提高决策树学习的效率。

**特征选择的准则是信息增益（information gain）或信息增益比**。

**熵（entropy）是表示随机变量不确定性的度量**。设$X$是一个取有限个值的离散随机变量，其概率分布为：
$$
P(X=x_i)=p_i, i=1,2,\cdots,n
$$
那么随机变量$X$的熵定义为：
$$
H(X)=-\sum_{i=1}^np_i\log p_i
$$
**熵只依赖于$X$的分布，而与$X$的取值无关，熵的单位可分为比特（bit，对数以2为底）和纳特（nat，对数以e为底）**。熵越大，随机变量的不确定性越大，$0\le H(p)\le\log n$。

当随机变量$X$服从伯努利（二项）分布时，熵$H(P)$随概率$P(X=0)=p$的变化如下

![image-20220503192151303](http://qiniu.lianghao.work/markdown/image-20220503192151303.png)

**条件熵（conditional entropy）$H(Y|X)$**表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。条件熵被定义为**X给定条件下Y的条件概率分布的熵对X的数学期望**
$$
H(Y|X)=\sum_{i=1}^nH(Y|X=x_i)*P(X=x_i),\ i=1, 2,\cdots,n
$$

$$
\begin{aligned}
H(Y|X=x_1) &= -\sum_{i=1}^n p(y_i|x_1)\log p(y_i|x_1)\\
H(Y|X=x_2) &= -\sum_{i=1}^n p(y_i|x_2)\log p(y_i|x_2)\\
&\vdots\\
H(Y|X=x_n) &= -\sum_{i=1}^n p(y_i|x_n)\log p(y_i|x_n)\\
\therefore H(Y|X) &= H(Y|X=x_1)*p(X=x_1)+\cdots+H(Y|X=x_n)*p(X=x_n)\\
&=\sum_{i=1}^n p(x_i)\cdot H(Y|x_i)

\end{aligned}
$$

当**熵和条件熵**中的概率由数据估计（极大似然估计）得到时，所对应的熵与条件熵分别称为**经验熵 (empirical entropy)**和**经验条件熵(empirical conditional entropy)**。

**信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度**。

#### 定义 5.2（信息增益）

**特征$A$对训练数据集$D$的信息增益$g(D, A)$，定义为集合$D$的经验熵$H(D)$与特征A给定条件下的$D$的经验条件熵$H(D|A)$之差**，即
$$
g(D,A)=H(D)-H(D|A)
$$
一般来说， **熵$H(Y)$与条件熵$H(Y|X)$**之差称为**互信息(mutual information)**，**决策树学习中的信息增益等价于训练数据中类与特征的互信息**。

如果数据集D表示类别，$A$表示特征。对于数据集D来说，信息增益依赖于特征。有了特征A，不确定性会降低，信息才有所增益。**不同特征对于同一数据集往往具有不同的信息增益，信息增益大的特征具有更强的分类能力。**

根据**信息增益准则**的特征选择方法：对训练数据集（或子集）D，计算每个特征的信息增益，并比较每个特征的信息增益，并比较他们的大小，选择信息增益最大特征。

*假设训练集为D，$|D|$表示样本容量（样本个数），由$K$个类$C_K$，$|C_K|$表示类属于$C_K$的样本个数，$\sum_{i=1}^K|C_i|=|D|$。假设特征$A$有$n$个不同的取值$\{a_1,a_2,\cdots,a_n\}$，根据特征$A$的取值将数据集D划分为n个子集，$\sum_{i=1}^n |D_i|=|D|$，记子集$D_i$中类属于$C_K$的样本的集合为$D_ik$，即$D_ik=D_i\cap C_K$*，信息增益的算法如下：

#### 算法 5.1 （信息增益的算法）

**输入：训练数据集$D$和特征$A$**

**输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$**

（1） 计算数据集$D$的经验熵（empirical entropy）$H(D)$
$$
H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2 \frac{|C_k|}{|D|}
$$
（2）计算特征A对数据集$D$的经验条件熵$H(D|A)$
$$
\begin{aligned}
H(D|A) &=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)\\
&=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_ik|}{|D_i|}\log_2\frac{|D_ik|}{|D_i|}
\end{aligned}
$$
（3）计算信息增益（information gain）
$$
g(D, A)=H(D)-H(D|A)
$$

### 5.2.3 信息增益比

以*信息增益*作为划分训练数据集的特征，存在**偏向于选择取值较多的特征**的问题。使用**信息增益比(information gain ratio)**可以对这一问题进行校正。这是特征选择的另一准则。

#### 定义 5.3 （信息增益比）

**特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D关于特征$A$的值的熵$H_A(D)$之比**
$$
g_R(D, A)=\frac{g(D, A)}{H_A(D)}\\
H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}
$$


##  5.3 决策树的生成

### 5.3.1 ID3 算法

ID3算法的核心是**在决策树各个结点上应用信息增益准则选择特征，从而递归地构建决策树**。具体方法是：从根节点（root node）开始，对结点计算所有可能的特征的信息增益，**选择信息增益最大的特征作为结点的特征**，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。**ID3相当于用极大似然法进行概率模型的选择**。

#### 算法 5.2 （ID3 算法）

输入：训练数据集$D$，特征集$A$阈值$\epsilon$

输出：决策树$T$

（1）若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将类$C_K$作为该结点的类标记，返回$T$

（2）若$A=\empty$，则$T$为单结点树，并将$D$中**实例数最大**的类$C_k$作为该结点的类标记，返回$T$；否则，按照**信息增益算法**计算$A$中各特征$A_i$对$D$的信息增益，选择信息增益最大的特征$A_g$

（3）如果$A_g$的信息增益小于阈值$\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$；否则，对$A_g$的每一个可能取值$a_i$，依照$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点后成树$T$，返回$T$

（4）对于第$i$个子结点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归地调用**步骤(1)~(3)**，得到子树$T_i$，返回$T_i$

### 5.3.2 C4.5 生成算法

C4.5算法在ID3算法上进行了改进：**使用信息增益比代替信息增益来选择当前最优特征**

## 5.4 决策树的剪枝

决策树生成算法递归地产生决策树，直到不能继续下去为止，这样产生的树对训练数据的分类很准确，但对为止的测试数据的分类往往不准确，即出现**过拟合现象**。

在决策树学习中将已生成的树进行简化的过程称为**剪枝 pruning**。

决策树的剪枝往往通过**极小化决策树整体的损失函数或代价函数**来实现。设树$T$的叶结点个数为$|T|$，$t$是$T$的叶结点，该叶结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk},\ k=1,2, 3,\cdots, K$个，$H_t(T)$为叶结点$t$上的经验熵，$\alpha\ge 0$为参数，则决策树学习的损失函数为
$$
\begin{aligned}
C_\alpha(T) &= \sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|\\
其中\ \ H_t(T) &=-\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}\\
令\  C(T) &= -\sum_{t=1}^{|T|}\sum_k^KN_{tk}\log\frac{N_{tk}}{N_t}\\
所以\ \ C_\alpha(T)&=C(T)+\alpha|T|
\end{aligned}
$$

其中，$C(T)$表示模型对训练数据的**预测误差**，即**模型与训练数据的拟合程度**， $|T|$表示**模型复杂度**，参数$\alpha\ge0$控制两者之间的影响。

**注**：*决策树生成只考虑了通过提高信息增益或信息增益比对训练数据进行更好的拟合，而决策树剪枝通过优化损失函数还考虑了减小模型复杂度*。**决策树生成学习局部的模型，而决策树剪枝学习整体的模型**。

剪枝，**就是$\alpha$确定时，选择损失函数最小的模型，即损失函数最小的子树**。当$\alpha$确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高，反之则反。损失函数正好表示了对**拟合效果**和**模型复杂度**的平衡。

#### 算法 5.4 （树的剪枝算法）

输入：生成算法产生的整个树$T$，参数$\alpha$

输出：修建后的子树$T_a$

（1）计算每个结点的**经验熵**

（2）**递归地从树地叶结点向上回缩**，设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B,T_A$，其对应的**损失函数值**分别是$C_\alpha(T_B),C_\alpha(T_A)$，如果二者满足：
$$
C_\alpha(T_A)\le C_\alpha(T_B)
$$
则进行**剪枝**，即将**父结点变为新的叶结点**。

（3）返回（2），直至不能继续为止，得到损失函数最小的子树$T_\alpha$

**注：比较回缩前后整体树的损失值大小，可以只需计算二者的损失函数之差，并且计算可以在局部进行。决策树的剪枝算法可以有一种动态规划的算法实现**。

## 5.5 CRAT 算法

**分类与回归树（classification and regression tree, CART）**模型由**Breiman**等人提出，是应用广泛的决策树学习方法。**CRAT由特征选择、树的生成以及生成树的剪枝组成，既可以用于分类也可以用于回归**。

CART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。**CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支**，这样的决策树等价于二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布。**CART算法由以下两步组成：**

（1）**决策树生成**：基于训练数据集生成决策树，**生成的决策树要尽量大**。

（2）**决策树剪枝**：用验证数据集对已生成的树进行剪枝并选择**最优子树**，这时使用**损失函数最小**作为剪枝的标准。

### 5.5.1 CART 生成

决策树的生成就是递归地构建二叉决策树的过程，对**回归树**用**平方误差最小化准测**，对**分类树**使用**基尼指数（Gini Index）最小化准则**，进行特征选择，生成二叉树。

#### 1. 回归树的生成

假设$X,Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集$D=\{(x_1,y_1),(x_2,y_2), \cdots,(x_N,y_N)\}$

一颗回归树对应着**输出（或特征）空间**的一个划分以及在*划分的单元*上的输出值。假设已将输入空间划分为$M$个单元$R_1,R_2,\cdots,R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是**回归树模型**可以表示为：
$$
\begin{aligned}
f(x)=\sum_{m=1}^Mc_mI(x\in R_m)
\end{aligned}
$$
当输入空间的划分确定是，可以使用**平方误差$\sum_{x_i\in R_m}(y_i-f(x_i))^2$**来表示回归树对于训练数据的**预测误差**，用**平方误差最小的准则**求解每个单元上的**最优输出值**。单元$R_m$上的$c_m$的最优值$\hat c_m$是$R_m$上**所有输入实例$x_i$对应的输出$y_i$的均值**：
$$
\begin{aligned}
\hat c_m = ave(y_i|x_i\in R_m)
\end{aligned}
$$
使用**启发式**的方法对**输入空间**进行**划分**：选择第$j$个变量$x^{(j)}$和它取的值$s$，作为**切分变量( splitting variable )**和**切分点(splitting point)**，并定义两个区域：
$$
\begin{aligned}
R_1(j,s)=\{x|x^{(j)}\le s\}\ \ \ and\ \ \ R_2(j,s)=\{x|x^{(j)}> s\}
\end{aligned}
$$
然后，寻找**最优切分变量$j$**和**最优切分点$s$**。即，求解:
$$
\begin{aligned}
\mathop {\min }\limits_{j,s} \left[\mathop {\min}\limits_{c_1}\sum_{x_i\in R_1(j, s)}(y_i-c_1)^2+\mathop {\min}\limits_{c_2}\sum_{x_i\in R_2(j, s)}(y_i-c_2)^2\right]
\end{aligned}
$$
对固定输入变量$j$可以找到最优切分点$s$
$$
\begin{aligned}
\hat c_1= ave(y_i|x_i\in R_1(j, s))\ \ and\ \ \hat c_2=ave(y_i|x_i \in R_2(j, s))
\end{aligned}
$$
遍历所有输入变量，找到最优的切分变量$j$，构成一个对$(j, s)$。依次将输入空间划分为两个区域。

对每个区域$R_m$重复上述的划分过程，直到满足条件为止。**这样生成的回归树通常称为最小二乘回归树 (least squares regression tree)**。

#### 算法 5.5 （最小二乘回归树生成算法）

输入：训练数据集$D$

输出：回归树$f(x)$

在训练数据集所在的输入空间中，**递归地将每个区域划分为两个子区域并决定每个子区域的输出值，构建二叉决策树**：

（1）选择**最优切分变量$j$**与**最优切分点$s$**，求解
$$
\begin{aligned}
\mathop {\min }\limits_{j,s} \left[\mathop {\min}\limits_{c_1}\sum_{x_i\in R_1(j, s)}(y_i-c_1)^2+\mathop {\min}\limits_{c_2}\sum_{x_i\in R_2(j, s)}(y_i-c_2)^2\right]
\end{aligned}
$$
**遍历变量$j$**，对**固定的切分变量$j$**扫描**切分点$s$**，选择使上式达到**最小值**的对$(j,s)$。

（2）用选定的**最优切分向量和最优切分点$(j,s)$**划分区域，并决定相应的输出值：
$$
\begin{aligned}
R_1(j,s)&=\{x|x^{(j)}\le s\}\ \ \ and\ \ \ R_2(j,s)=\{x|x^{(j)}> s\}\\
\hat c_m &=\frac{1}{N_m}\sum_{x_i \in R_m} y_i,\ \ x \in R_m,\ \ m=1, 2
\end{aligned}
$$
（3）**继续对两个子区域**调用步骤 (1)，(2)，直到满足停止条件

（4）将**输入空间**划分为$M$个区域$R_1,R_2,\cdots,R_M$，生成决策（回归）树。
$$
\begin{aligned}
f(x)=\sum_{m=1}^M\hat c_mI(x\in R_m)
\end{aligned}
$$

#### 2. 分类树的生成

分类树用**基尼指数**选择最优特征，同时决定该特征的**最优二值切分点**。

#### 定义 5.4 （基尼指数）

分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则**概率分布的基尼值**定义为：
$$
\begin{aligned}
Gini(p) = \sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^K p_k^2
\end{aligned}
$$
对于*二分类问题*，若样本点属于*第1个类*的概率是$p$，则概率分布的基尼值为：
$$
\begin{aligned}
Gini(p)&=1- \sum_{k=1}^2p_k^2\\
&=1-p_1^2-p_2^2\\
&=1-p^2-(1-p)^2\\
&=2p(1-p)
\end{aligned}
$$
对于给定的样本集合$D$，其基尼值为：
$$
Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2
$$
这里，$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。

**样本集合D**根据**特征A**是否取某一可能值$a$被分割为$D_1$和$D_2$两部分，即
$$
\begin{aligned}
D_1=\{(x,y)\in D|A(x)=a\}, \ D_2=D-D_1
\end{aligned}
$$
则在特征$A$的条件下，集合$D$的**基尼指数**定义为：
$$
\begin{aligned}
Gini(D)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
\end{aligned}
$$
**基尼指数$Gini(D)$**表示集合$D$的**不确定性**，$Gini(D,A)$表示经$A=a$分割后集合$D$的不确定性。

![](http://qiniu.lianghao.work/markdown/20220505145114.png)

#### 算法 5.6 （CART生成算法）

输入：训练数据集$D$，停止计算的条件

输出：CART决策树

*根据训练数据集，从根节点开始，递归地对每个结点进行以下操作，构建二叉决策树*

（1）设结点的训练数据集为$D$，计算现有特征对该数据集的基尼指数。此时，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或“否”，将$D$分割为$D_1,D_2$两个部分，计算基尼指数$Gini(D,A)$。

（2）在所有可能的**特征**$A$以及它们所有可能的**切分点**$a$中，选择**基尼指数$Gini(D,A)$最小**的特征及其对应的切分点作为**最优特征**和**最优切分点**。根据选择的最优特征和最优切分点，**从现结点中生成两个子结点**，将训练数据集依据特征分配到两个子结点中。

（3）对两个子结点递归地调用(1) ，(2)。直至满足停止条件

（4）生成CART决策树

**注：**

**1. 算法的停止计算条件是结点中的样本个数小于预定的阈值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或没有更多的特征**。

**2. 如果特征$A$的取值个数为2时，根据Gini计算公式，特征$A$取任何一个值$a$，计算得到的Gini指数时相同的，即对于二分类的特征来说，其切分点可以为任何一个取值**。对于有**多个分类值**的特征，需要计算每一个取值的Gini指数，然后取Gini指数最小的值$a$作为特征A的切分点。

### 5.5.2 CART剪枝

CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小（模型变简单），从而能够对为止数据有更准确的预测。

**CART剪枝算法由两部组成：**

1. 首先从**生成算法**产生的决策树$T_o$**底端**开始不断剪枝，直到$T_0$的**根节点**，形成一个子树序列$\{T_0,T_1,\cdots, T_n\}$；
2. 然后，通过**交叉验证法**在**独立的验证数据集**上对子树序列进行测试，从中选择最优子树。

#### 1. 剪枝，形成一个子树序列

剪枝过程中，计算子树的损失函数：
$$
\begin{aligned}
C_\alpha(T)=C(T)+\alpha|T|
\end{aligned}
$$
其中，$T$为任意子树，$C(T)$为**对训练数据的预测误差**（如基尼指数），$|T|$对叶子结点个数。

对于固定的$\alpha$，**一定存在使损失函数$C_\alpha(T)$最小的子树，将其表示为$T_\alpha$。$T_\alpha$在损失函数$C_\alpha(T)$最小的意义下是最优的子树**。当$\alpha$大的时候，最优子树$T_\alpha$偏小；当$\alpha$小的时候，最优子树$T_\alpha$偏大。极端情况下，$\alpha=0$时，整体树是最优的。当$\alpha \rightarrow \inf$时，根结点组成的单结点树是最优的。

**Breiman等人证明，可以使用递归的方法对树进行剪枝**。

具体地，从整体树$T_0$开始剪枝，对$T_0$的**任意内部结点$t$**，以$t$为**单结点树的损失函数**是
$$
\begin{aligned}
C_\alpha(t)=C(t)+\alpha
\end{aligned}
$$
以$t$为根节点的子树$T_t$的损失函数是
$$
C_\alpha(T_t)=C(T_t)+\alpha|T_t|
$$
当$\alpha=0$及$\alpha$充分小时，有不等式：$C_\alpha(T_t)<C_\alpha(t)$

当$\alpha$增大时，在某一处$\alpha$有：$C_\alpha(T_t)=C_\alpha(t)$

当$\alpha$继续增大时，$C_\alpha(T_t)>C_\alpha(t)$。此时，只要$\alpha=\frac{C(t)-C(T_y)}{|T_t-1|}$，$T_t,\ t$有相同的损失函数值，然而，$t$只有一个结点，因此$t$比$T_t$更可取，因此，对$T_t$进行剪枝。

**对整体树$T_0$中的每一个内部结点$t$，计算**
$$
\begin{aligned}
g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}
\end{aligned}
$$
这表示剪枝后的整体损失函数减少的程度。在$T_0$中减去$g(t)$**最小**的$T_t$，将得到的子树作为$T_1$，同时将**最小的$g(t)$**设为$\alpha_1$。$T_1$为区间$[\alpha_1,\alpha_2]$的最优子树。

如此继续剪枝下去，直到得到根节点。

#### 2. 在剪枝得到的子树序列$T_0,T_1,\cdots,T_n$中通过交叉验证选取最优子树$T_\alpha$

利用独立的验证数据集，测试子树序列中各棵子树的**平方误差**或**基尼指数**，其中值最小的决策时被认为时最优决策树。在子树序列中，每个子树$T_k$都对应于一个参数$\alpha_k$，最优决策树$T_\alpha$

#### 算法 5.7 （CART剪枝算法）

输入：CART算法生成的决策树$T_0$

输出：最优决策树$T_\alpha$

（1）设$k=0,\ T=T_0$

（2）设$\alpha=+\inf$

（3）**自下而上**地对各内部结点$t$计算$C(T_i),|T_i|$以及
$$
\begin{aligned}
g(t) &= \frac{C(t)-C(T_t)}{|T_t|-1}\\
\alpha &= \min(\alpha,g(t))
\end{aligned}
$$
（4）对$g(t)=\alpha$的内部结点$t$进行剪枝，并对叶结点$t$以多数表决法决定其类，得到树$T$

（5）设$k=k+1,\alpha_k=\alpha, T_k=T$

（6）如果$T_k$不是由根结点及两个叶结点构成的树，则回到步骤（2），否则令$T_k=T_n$

（7）采用交叉验证法在子树序列$T_0, T_1, \cdots,T_n$中选取最优子树$T_\alpha$

#### 注：疑问

1. CART剪枝算法中的$C(T_t)$如何求？
2. 交叉验证如何使用











