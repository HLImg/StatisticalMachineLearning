# 第 4 章 朴素贝叶斯法

[例题与习题](https://github.com/aixiaoairen/StatisticalMachineLearning/blob/master/Codes/4.%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95.ipynb)：**<a href ="https://github.com/aixiaoairen/StatisticalMachineLearning/blob/master/Codes/4.%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95.ipynb"><img src="http://qiniu.lianghao.work/markdown/LiangH-StatiticalMachineLearning-blue"></a>**

**朴素贝叶斯（naive Bayes）法是基于贝叶斯定理与特征条件独立假设的分类方法**。对于给定的训练数据集，首先基于**特征条件独立假设**学习输入输出的**联合概率分布**；然后基于此模型，对给定的输入$x$，利用**贝叶斯定理**求出**后验概率最大的输出$y$**。

## 4.1 朴素贝叶斯的学习与分类

### 4.1.1 基本方法

设输入空间$X \subseteq R^n$为$n$维向量的集合，输出空间维类标记集合$Y = {c_1, c_2, \cdots, c_K}$。输入为特征向量$x\in X$，输出为类标记（class label）$y \in Y$。$X$是输入空间上的随机变量，$Y$是输出空间上的随机变量。$P(X,Y)$是$X,Y$的联合概率分布。训练数据集$T=\{(x_1, y_1),(x_2, y_2), \cdots,(x_N, y_N)\}$由$P(X,Y)$独立同分布产生。

1. **先验概率分布**：$P(Y=c_k), \ k=1, 2, 3,\cdots, K$
2. **条件概率分布**：$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)}, X^{(2)}=x^{(2)}, \cdots, X^{(n)}=x^{(n)}|Y=c_k),\ k=1, 2, \cdots, K$
3. 可以根据先验概率分布和条件概率分布学习到*联合概率分布$P(X,Y)$*。

条件概率分布$P(X=x|Y=c_k)$有指数级数量的参数，其估计实际是不可行的。假设$x^{(j)}$可能值有$S_j$个，$j=1, 2,\cdots,n$，$Y$可能取值有$K$个，那么参数的个数可能为$K\sum_{j=1}^n S_j$。

**朴素贝叶斯法**对条件概率分布作了**条件独立性**的假设：
$$
\begin{aligned}
P(X=x|Y=c_k) &= P(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)}|Y=c_k)\\
&=\prod\limits_{j=1}^n P(X^{(j)}|Y=c_k)
\end{aligned}
$$
*朴素贝叶斯法实际上学习到生成数据的机制，属于生成模型*。条件独立假设强制用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯法变得简单，**但有时也会牺牲一定的分类准确性**。

朴素贝叶斯法分类时，对于给定的输入$x$，通过学习到的模型**计算后验概率分布$P(Y=c_k|X=x)$，将后验概率最大的类作为$x$的类输出**。根据贝叶斯定理来计算后验概率：
$$
\begin{aligned}
P(Y=c_k|X=x) &= \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}
\end{aligned}
$$
将上述两式结合，可得到**朴素贝叶斯分类的基本公式**：
$$
\begin{aligned}
P(Y=c_k|X=x) &= \frac{P(Y=c_k)\prod\limits_{j=1}^nP(x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}
\end{aligned}
$$
**朴素贝叶斯分类器可以表示**：
$$
\begin{aligned}
y&=f(x) \\&=arg\ \mathop{max}\limits_{c_k} \ P(Y=c_k|X=x) \\&= \frac{P(Y=c_k)\prod\limits_{j=1}^nP(x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}
\end{aligned}
$$
在同一个问题，对于每个类别$c_k$，朴素贝叶斯分类公式的分母是相同的，因此，可以简化如下：
$$
\begin{aligned}
y&=f(x) \\&=arg\ \mathop{max}\limits_{c_k} \ P(Y=c_k|X=x) \\&= P(Y=c_k)\prod\limits_{j=1}^nP(x^{(j)}|Y=c_k)
\end{aligned}
$$

### 4.1.2 后验概率最大化的含义

**朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化**。假设损失函数为$0-1$损失函数：
$$
\begin{aligned}
L(Y,f(X)) = \left\{ \begin{array}{l}
1,\ Y\neq f(X)\\
0,\ Y=f(X)
\end{array} \right.
\end{aligned}
$$
其中，$f(X)$是分类决策函数。期望风险为：
$$
\begin{aligned}
R_{\exp}(f)=E[L(Y, f(X))]
\end{aligned}
$$
*期望是对联合分布$P(X,Y)$取的。由此，取条件期望*
$$
\begin{aligned}
R_{\exp}(f) &=\int_{X\cdot Y}{L(Y,f(X))P(X,Y)dXdY} \\
&=\int_{X\cdot Y}{L(Y,f(X))P(Y|X)P(X)dXdY} \\
&=\int_{X}P(X)[\int_Y{L(Y,f(X))P(Y|X)}dY]dX \\
&=E_X[\int_Y{L(Y,f(X))P(Y|X)dY}]\\
&=E_X\sum_{k=1}^K[L(c_k, f(X))]P(c_k|X)
\end{aligned}
$$
**为了使期望风险最小化，只需要对$X=x$逐个极小化，由此可得**
$$
\begin{aligned}
f(x)&=arg\ \mathop{\min}\limits_{y\in Y}\sum_{k=1}^K L(c_k,y)P(c_k|X=x)\\
&= arg\ \mathop{\min}\limits_{y\in Y}\sum_{k=1}^K P(y\neq c_k|X=x)\\
&=arg\ \mathop{\min}\limits_{y\in Y} (1-P(y=c_k|X=x))\\
&=arg\ \mathop{\max}\limits_{y\in Y}P(y=c_k|X=x)
\end{aligned}
$$
**根据期望风险最小化准则就得到了后验概率最大化准则**：
$$
\begin{aligned}
f(x) &= arg\ \mathop{max}\limits_{c_k}\ P(c_k|X=x)
\end{aligned}
$$

## 4.2 朴素贝叶斯法的参数估计

### 4.2.1 极大似然估计

在朴素贝叶斯法中，学习意味着估计$P(Y=c_k)$和$P(X^{(j)}=x^{(j)}|Y=c_k)$。可以使用**极大似然估计法**估计相应的概率。**先验概率$P(Y=c_k)$**的极大似然估计：
$$
\begin{aligned}
P(Y=c_k)=\frac{\sum_{i=1}^N I(y_i=c_k)}{N}, \ k=1,2,\cdots,K
\end{aligned}
$$
设第$j$个特征$x^{(j)}$可能取值的集合为$\{a_{j1}, a_{j2}, \cdots, a_{jS_j}\}$，条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计是：
$$
\begin{aligned}
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=a_k)}{\sum_{i=1}^N I(y_i=c_k)}
\end{aligned}
$$
其中，$j=1,2,\cdots,n;\ l=1,2,\cdots,S_j;\ k=1,2,\cdots,K$。$x_i^{(j)}$表示第$i$个样本的第$j$个特征；$a_{jl}$表示第$j$个特征可能取得第$l$个值；$I(\cdot)$为指示函数。

### 4.2.2 学习与分类算法

### 算法 4.1 （朴素贝叶斯算法（naive Bayes algorithm））

**输入**：训练数据$T=\{(x_1,y_1),(x_2, y_2),\cdots,(x_N,y_N)\}$，其中$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$，$x_i^{(j)}$是第$j$个样本的第$j$个特征，$x_i^j\in \{a_{j1},a_{j2},\cdots,a_{jS_j}\}$，其中$a_{jl}$是第$j$个特征可能取的第$l$个值，$j=1,2,\cdots,n,\ l=1,2,\cdots,S_j$，$,\ y_i \in\{c_1,c_2,\cdots,c_K\}$；实例$x$；

**输出**：实例$x$的分类。

（1）计算先验概率$P(Y)$以及条件概率$P(X|Y)$
$$
\begin{aligned}
P(Y=c_k) &= \frac{\sum_{i=1}^N I(y_i=c_k)}{N},\ k=1,2,\cdots,K\\
P(X^{j}=a_{jl}|Y=c_k) &= \frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}
\end{aligned}
$$
（2）对于给定的实例$x=(x^{(1)},x^{(2)},\cdots,x^{(n)})^T$，计算
$$
\begin{aligned}
P(Y=c_k)\prod\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k),\ k=1,2,\cdots,K
\end{aligned}
$$
（3）确定实例$x$的类
$$
\begin{aligned}
y &= arg\ \mathop{\max}\limits_{c_k}\ P(Y=c_k)\prod\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
\end{aligned}
$$

### 4.2.3 贝叶斯估计

*注：使用极大似然估计会出现所要估计的概率值为0的情况，这会影响后验概率的计算结果，使分类产生偏差*。解决这一问题的方法是采用**贝叶斯估计**。

**条件概率**的贝叶斯估计为：
$$
\begin{aligned}
P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i = c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}
\end{aligned}
$$
其中，$\lambda\ge0$，这等价于在每个取值的*频数*上赋予一个正数。$S_j$为第$i$特征向量$x_i^{(j)}$取值集合$a_j = \{a_{j1}, a_{j2},\cdots, a_{jS_j}\}$的大小。

当$\lambda=0$时，变为了**极大似然估计**。当$\lambda=1$时，称为**拉普拉斯平滑（Laplacian smoothing）**。

**先验概率**的贝叶斯估计：
$$
\begin{aligned}
P_\lambda(Y=c_k)=\frac{\sum_{i=1}^N I(y_i=c_k)+\lambda}{N + K\lambda}
\end{aligned}
$$
