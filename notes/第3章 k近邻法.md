# 第 3 章  k 近 邻 法

[例题与习题](https://github.com/aixiaoairen/StatisticalMachineLearning/blob/master/Codes/3.k%E8%BF%91%E9%82%BB%E6%B3%95.ipynb)：**<a href ="https://github.com/aixiaoairen/StatisticalMachineLearning/blob/master/Codes/3.k%E8%BF%91%E9%82%BB%E6%B3%95.ipynb"><img src="http://qiniu.lianghao.work/markdown/LiangH-StatiticalMachineLearning-blue"></a>**

**k 近邻法（k-nearset neighbor， k-NN）是一种基本的分类与回归方法**。k近邻法的输入为**实例的特征向量，对应于特征空间的点**，输出为**实例的类别，可以取多类**。

k近邻法假设给定一个训练数据集，其中每个实例的类别都已定。*分类时，对于新的实例，根据其$k$个最近邻的训练实例的类别，通过多数表决等方式进行预测*。k近邻法**不具有显示的学习过程**，**实际上就是利用训练数据集对特征向量空间进行划分，并作为其分类的模型**。

**k近邻法的三要素是：k值的选择、距离度量及分类决策规则**。

## 3.1 k 近邻算法

*给定一个训练数据集，对于新的输入实例，在数据集中找到与该实例最邻近的k个实例。这个k个实例的多数属于某个类，就把该输入实例分为这个类。*

### 算法 3.1 （k近邻法）

**输入**：训练数据集  $T=\{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}$，其中，$x \in X \subseteq R^n$为实例的特征向量，$y_i \in Y=\{c_1, c_2,\cdots, c_K\}$为实例的类别，$i=1, 2, 3, \cdots, N$。

**输出**：实例$x$所属于的类$y$。

（1）根据给定的**距离度量**，在训练集$T$中找出与$x$最近邻的$k$个点，涵盖这$k$个点的$x$的邻域记作$N_k(x)$；

（2）在$N_k(x)$中根据**分类决策规则（例如多数表决）**决定特征向量$x$的类别$y$：
$$
\begin{aligned}
y = arg\ \mathop {max}\limits_{c_j}\sum_{x_i \in N_k(x)}I(y_i=c_j),\ \ \ \ i=1,2,\cdots, N;\ j=1, 2, \cdots,K
\end{aligned}
$$
其中，$I$为**指示函数**，即当$y_i = c_j$时，$I=1$，否则$T=0$。

当$k=1$时，称之为**最近邻算法**。此外，**k近邻法没有显示的学习过程**。

## 3.2 k 近邻模型

**模型的三个基本要素——距离度量、k值的选择和分类决策的决定**。

### 3.2.1 模型

特征空间中，对于每个训练实例的点$x_i$，**距离该点比其他点更近的所有点**组成一个区域，叫做**单元（cell）**。每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。

### 3.2.2 距离度量

**特征空间中两个实例点的距离时两个实例点相似程度的反映**。k近邻模型的特征空间一般是$n$维实数向量空间$R^n$，使用的距离是欧式距离，但也可以是其他距离，如**$L_p$距离（$L_p$ distance）或 Minikowski（Miniknowski distance）**。

设特征空间$X$是$n$维实数向量空间$R^n, \ x_i, x_j \in X, \ x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T, \ x_j=(x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)})^T$, **实例点$x_i,x_j$的$L_p$距离定义如下**：
$$
L_p(x_i, x_j)=\left(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p\right)^{\frac{1}{p}}
$$

1. $p=1$时，称为**曼哈顿距离（Manhattan distance）**
2. $p=2$时，称为**欧式距离（Euclidean distance）**
3. $p=\infty$时，是**各个坐标距离的最大值**

![image-20220414121851105](http://qiniu.lianghao.work/markdown/image-20220414121851105.png)

### 3.2.3 k 值的选择

如果选择**较小的k值**，相当于用较小的邻域中的训练实例进行预测，**优点是“学习”的近似误差（approximation error）会减小，但是**“学习”的*估计误差（estimation error）会增大*，预测结果会对近邻的实例点非常敏感**，当邻近的实例点恰巧试噪声时，预测就会出错**。**k值的减小意味着整体模型变得复杂，容易发生过拟合**。

如果选择**较大得k值**，相当于用较大得邻域中的训练实例进行预测，**优点是可以减少学习得估计误差**，但**缺点是学习得近似误差会增大**，与实例较远（不相似的）训练实例也会对预测进行干扰。**k值的增大意味着整体的模型变得简单**。

注：在应用中，*k值一般取一个比较小的数值*，通常**采用交叉验证法**来**选取最优的k值**。

### 3.2.4 分类决策规则

多数表决规则（majority voting rule）：如果分类的损失函数为$0-1$损失函数，分类函数为：
$$
f:R^n \rightarrow \{c_1, c_2, \cdots, c_K\}
$$
误分类的概率是：
$$
P(Y \neq f(X))=1-P(Y=f(X))
$$
对于给定的实例$x\in X$，其最近邻的$k$个训练实例点构成集合$N_k(x)$。如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是：
$$
\frac{1}{k}\sum_{x_i \in N_k(x)}I(y_i \neq c_j)=1 - \frac{1}{k}\sum_{x_i \in N_k(x)}I(y_i = c_j)
$$
若使**误分类率**最小即**经验风险**最小，就要使$\sum_{x_i \in N_k(x)}I(y_i = c_j)$最大，所以多数表决规则等价于**经验风险最小化**。

## 3.3 k 近邻法的实现：$kd$树

实现$k$近邻时，主要考虑的问题是如何对训练数据进行**快速k近邻搜索**。

为了提高k近邻搜索的效率，*可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数*。

### 3.3.1 构造kd树（kd tree）

**kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构，kd树是二叉树，表示对k维空间的一个划分（partition）**。构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。**kd树的每个结点对应于一个k维超矩形区域**。

构造kd树的方法如下：*构造根节点，使根节点对应于k维空间中包含所有实例点的超矩形区域*。通过下面的递归方法，不断地对k维空间进行切分，生成子节点。在*超矩形区域（节点）上选择*一个*坐标轴*和在此坐标轴上的*一个切分点*，确定*一个超平面*，这个超平面通过选定的切分点并垂直于选择的坐标轴，将当前超矩形区域切分为两个子区域（子节点）；此时，实例被分到两个子区域，这个过程直到子区域没有实例时终止（终止时的节点为叶节点）。

**注**：通常，依次选择坐标轴对空间切分，**选择训练实例点在选定坐标轴上的中位数 media**作为切分点，这样得到的kd树是**平衡的**。但是**平衡的kd树搜索时的效率未必是最优的**。

### 算法 3.2 （构造平衡 *kd* 树）

**输入**：$k$维空间数据集 $T=\{x_1, x_2, \cdots, x_N\}$，其中$x_i = (x_i^{(1)},x_i^{(2)},\cdots, x_i^{(k)})^T,\ i=1,2,\cdots, N$。

**输出**：$kd$树

（1）开始： 构造根结点，**根结点对应于包含$T$的$k$维空间的超矩形区域**。

选择$x^{(1)}$为坐标轴，以$T$中所有实例的$x^{(1)}$坐标的**中位数**作为切分点，将根节点对应的超矩形区域切分为两个子区域。**切分由通过切分点并于坐标轴$x^{(1)}$垂直的超平面实现**。

由根节点生成深度为1的左右子结点：*左子结点对应$x^{(1)}$坐标小于切分点的子区域*，*右子结点对应$x^{(1)}$坐标大于切分点的子区域*。

**将落在切分超平面上的实例点保存在该节点**。

（2）重复：对于深度为$j$的结点，选择$x^{(l)}$为切分的坐标轴，$l=j(\mod \ k) + 1$，以该结点的区域中所有实例的$x^{(l)}$坐标的中位数作为切分点，将该结点对应的超矩形区域划分为两个子区域。切分由通过切分点$media(x^{(l)})$并与坐标轴$x^{(l)}$垂直的超平面实现。

由该结点生成深度为$j+1$的左、右子结点：*左子结点对应$x^{(l)}$坐标小于切分点的子区域*，*右子结点对应$x^{(l)}$坐标大于切分点的子区域*。

**将落在切分超平面上的实例点保存在该节点**。

（3）直到两个子区域没有实例存在时停止，从而形成kd树的区域划分。

### 3.3.2 搜索$kd$树

使用**kd**树进行**k近邻搜索**可以省去对大部分数据点的搜索，从而减少搜索的计算量。

**最近邻kd树搜索**：给定一个目标点，搜索其最近邻点。**首先找到包含目标点的叶结点**；然后从该**叶结点**出发，依次回退到**父节点**；不断*查找与目标点最邻近的结点*，当确定不存在更近的结点时。这样的搜索就被限制在空间的局部区域上，效率大为提高。

### 算法 3.3 （用kd树的最近邻搜索）

**输入**：已构造的kd树，目标点$x$

**输出**：$x$的最近邻

（1）在$kd$树中找出包含目标点$x$的叶节点：从根节点出发，递归地向下访问$kd$树。若目标点$x$当前维的坐标小于切分点的坐标，则移动到**左子结点**，否则移动到**右子结点**。**直到子结点为叶结点为止**。

（2）以此**叶结点**为**当前最近点**。

（3）**递归地向上回退**，在每个结点进行以下操作：

- （a）如果该结点*保存的实例点*比当前**最近点**距离目标点**更近**，则以该实例点为**当前最近点**。
- （b）当前最近点一定**存在于该结点一个子结点对应的区域**。检查**该子结点的兄弟结点对应的区域是否有更近的点**。具体地，检查兄弟节点对应区域*是否存在结点*到**目标点**地距离比**目标点到当前最近点**的距离要近，如果存在这样的结点，说明兄弟结点对应的区域可能存在距目标点更近的点，移动到这个兄弟结点上。接着，递归地进行最近邻搜索。

（4）当回退到根结点时，搜索结束。最后的**当前最近点**就是$x$的最近邻点。

*因为实例点时随机分布的，$kd$树搜索的平均计算复杂度为$O(\log N)$，这里$N$是训练实例树。$kd$树更适用于训练实例N远大于空间维数时的k邻近搜索，当二者接近时，效率会迅速下降，几乎接近线性扫描*。



