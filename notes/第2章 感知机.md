# 第 2 章 感知机

[例题与习题](https://github.com/aixiaoairen/StatisticalMachineLearning/blob/master/Codes/2.%E6%84%9F%E7%9F%A5%E6%9C%BA.ipynb)：**<a href ="https://github.com/aixiaoairen/StatisticalMachineLearning/blob/master/Codes/2.%E6%84%9F%E7%9F%A5%E6%9C%BA.ipynb"><img src="http://qiniu.lianghao.work/markdown/LiangH-StatiticalMachineLearning-blue"></a>**

**感知机（perceptron）是二类分类的线性模型**，其输入为实例的特征向量，输出为实例的类型，取$\{-1, +1\}$二值。感知机对应于输入空间（特征空间）中将实例划分为**正负两类的分离超平面，属于判别模型**。*感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行最小化，求得感知机模型。*感知机学习算法具有简单而易于实现的特点，分为**原始形式**和**对偶形式**。

## 2.1 感知机模型

### 定义 2.1（原始形式的感知机学习算法）

**假设输入空间（特征空间）是$X \subseteq  R^n$，输出空间是$y={-1,1}$，输入$x\in X$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y \in Y$表示实例的类别，由输入空间到输出空间的函数如下：**
$$
f(x)=sign(w\cdot x+b)
$$
该函数称为**感知机**。其中$w$和$b$为感知机模型参数，$w \in R^n$叫做权值（weight）或者权值向量（weight vector）,$b\in R$叫做偏置（bias），$w \cdot x$表示$w,x$的内积，$sign(\cdot)$是符号函数。
$$
s = \left\{ \begin{array}{l}
+1, \ \ x\ge0\\
-1,\ \ x<0
\end{array} \right.
$$
感知机是一种线性分类模型，属于判别模型。感知机模型的*假设空间*是定义在*特征空间*中所有线性分类模型（linear classification）或线性分类器（linear classifier），即函数集合$\{f|f(x)=w\cdot x+b\}$。

感知机的**几何解释**：线性方程$w\cdot x + b = 0$对应于特征空间$R^n$的一个**超平面$S$**，其中$w$是超平面的*法向量*，$b$是超平面的载距。这个*超平面*将特征空间划分为两个部分。位于两部分的点（特征向量）分别被分为*正负两类*。称超平面$S$为**分离超平面（separate hyper plane）**

![image-20220412173745870](http://qiniu.lianghao.work/markdown/image-20220412173745870.png)

## 2.2 感知机学习策略

### 2.2.1  数据集的线性可分性

###  定义2.2  数据集的线性可分性

给定一个数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，其中，$x_i \in R^n, y_i \in Y={+1, -1}, i=1, 2, \cdots, N$，如果存在某个超平面$S: \ \ w\cdot x+b = 0$能够将数据集的**正实例点**和**负实例点**完全正确地**划分到超平面的两侧**，即对*所有$y_i=+1$的实例，都有$w \cdot x_i + b >0$*，对*所有$y_i=-1$的实例，都有$w \cdot x_i + b <0$*，**则称数据集T线性可分数据集（linearly separable data set）**。

### 2.2.2 感知机学习策略

假设训练数据集是**线性可分的**，感知机学习的**目标是求得一个能够将训练集正实例点和负实例点完全正确分开的超平面，即确定$w, b$**。

学习策略（损失函数）的一个自然的选择是*误分类点的总数*，然而，这样的损失函数不是*参数$w,b$的连续可导函数，不容易优化*。**感知机选择的学习策略（损失函数）是误分类点到超平面S的总距离**。

输入空间$R^n$中任意一点$x_0$到超平面$S$的距离:
$$
\frac{1}{||w||}|w \cdot x_0 + b|
$$
对于误分类数据$(x_i,y_i)$来说，$-y_i(w \cdot x_i + b) >0$，误分类点$x_i$到超平面$S$的距离为
$$
-\frac{1}{||w||}y_i(w \cdot x_1 + b)
$$
假设超平面$S$的*误分类点集合为M*，那么所有误分类点到超平面$S$的总距离为：
$$
-\frac{1}{||w||}\sum_{i=1}^Ny_i(w \cdot x_1 + b)
$$
不考虑$\frac{1}{||w||^2}$，就得到了感知机学习的损失函数：
$$
L(w, b)=-\sum_{x_i\in M}y_i(w\cdot x_i +b)
$$
其中，$M$为**误分类点**的集合，这个损失函数就是感知机学习的**经验风险函数**。

## 2.3 感知机学习算法

**感知机学习问题转化为求解感知机损失函数的最优化问题，最优化方法是随机梯度下降法。**

### 2.3.1 感知机学习算法的原始形式

感知机学习算法是对以下优化问题的算法。给定一个训练集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，其中，$x_i\in X=R^n, \ y_i \in Y=\{-1, 1\}, i=1, 2, \cdots, n$，求参数$w, b$，使其为以下损失函数极小化的解：
$$
\mathop {\min}\limits_{w,b} {L(w, b)=-\sum_{x_i \in M}y_i(w\cdot x_i+b)}
$$
感知机学习算法是*误分类*驱动的，采用**随机梯度下降（stochastic gradient descent）**。首先，任意选取一个超平面$S(w_0, b_0)$，然后用**梯度下降法不断地极小化目标函数**。*极小化的过程不是一次使M中所有误分类点的梯度下降，而是一次随机选择一个误分类点使其梯度下降*。

假设误分类点集合$M$是固定的，损失函数$L(w, b)$的梯度是：
$$
\begin{aligned}
\nabla_wL(w, b) &= -\sum_{x_i \in M}x_iy_i\\
\nabla_bL(w, b)&=-\sum_{x_i \in M}y_i
\end{aligned}
$$
**随机选取一个误分类点$(x_i, y_i)$对$w, b$进行更新**：
$$
w \leftarrow w + \eta \cdot y_ix_i\\
b \leftarrow b + \eta \cdot y_i
$$
其中，$\eta(0<\eta\le1)$是步长，也称为**学习率（learning rate）**，这样通过迭代可以期待损失函数不断减小，直到为0。

#### 算法 2.1（感知机学习算法的原始形式）

**输入：**训练数据集$T=\{(x_1, y_1),(x_2, y_2), \cdots,(x_N, y_N)\}$，其中$x_i \in R^n, y_i \in \{-1, +1\}, i = 1, 2, \cdots, N$, 学习率为$\eta(0<\eta\le 1)$；

**输出：**$w, b$：感知机模型$f(x)=sign(w\cdot x+b)$

(1)  选取初值，$w_0, b_0$；

(2)  在训练集中选取数据$(x_i, y_i)$；

(3)  如果$y_i(w \cdot x_i + b)\le 0$，*表示该点被误分类了*：
$$
w \leftarrow w + \eta\cdot y_ix_i\\
b \leftarrow b + \eta\cdot y_i
$$
(4) 转至（2），直至训练集中没有误分类点。

*注： 该算法是感知机学习的基本算法，对应于对偶形式，称为原始形式*。

感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同。

### 2.3.2 算法的收敛性

性质：*对于线性可分数据集，感知机学习算法原始形式收敛，即经过有限次迭代后，可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型*。

####  定理 2.1（Novikoff）

**假设训练数据集$T=\{(x_1, y_1),(x_2, y_2),\cdots, (x_N, y_N)\}$是线性可分的，其中$x_i \in X= R^n, y_i \in Y=\{-1,+1\}$**

1. *存在满足条件$||\hat w_{opt}||=1$的超平面$\hat w_{opt}\cdot \hat x=w_{opt}\cdot x + b_{opt}=0$将训练数据集完全正确分开；且存在$\gamma>0$，对所有$i=1,2,\cdots,N$*
   $$
   y_i(\hat w_{opt}\cdot \hat x)=y_i(w_{opt}\cdot x + b_{opt})\ge\gamma
   $$

2. *令$R=\mathop {\max}\limits_{1\le i\le N}||\hat x_i||$，则原始感知机算法在训练数据上的误差分类次数k满足不等式*
   $$
   k \le \left(\frac{R}{\gamma}\right)^2
   $$
   其中，$\hat w=(w^T, b)^T, \hat x=(x^T, 1)^T$，即$\hat w \cdot \hat x = w \cdot x + b$。

#### 证明 2.1（Novikoff）

1. 由于训练数据是**线性可分的**，即*存在超平面*可将训练数据集完全正确分开，取此超平面为$\hat w_{opt}\cdot \hat x=w_{opt} \cdot w + b_{opt}=0$，使得$||\hat w_{opt}||=1$。由于对于有限$i = 1, 2, \cdots,N$均有：
   $$
   y_i(\hat w_{opt} \cdot \hat x_i)=y_i(w_{opt}\cdot x_i + b_{opt})>0
   $$
   所以存在
   $$
   \gamma =\mathop {\min}\limits_{i}\{y_i(w_{opt}\cdot x_i)+b_{opt}\}
   $$
   使得
   $$
   y_i(\hat w_{opt} \cdot \hat x_i)=y_i(w_{opt}\cdot x_i + b_{opt})\ge\gamma
   $$
   
2. 感知机分类算法从$\hat w =0$开始。如果实例被误分类，则重新更新权重，令$\hat w_{k-1}$是第$k$个误分类实例之前的扩充权重向量，即
   $$
   \hat w_{k-1}=(w_{k-1}^T, b_{k-1})^T
   $$
   则第$k$个实例*被误分类*的条件为：
   $$
   y_i(\hat w_{k-1}\cdot \hat x)=y_i(w_{k-1}\cdot x_i + b_{k-1})\le 0
   $$
   若$(x_i, y_i)$是被$\hat w_{k-1}=(w_{k-1}^T, b_{k-1})^T$误分类的数据，则$w$和$b$的更新是
   $$
   w_k \leftarrow w_{k-1}+\eta y_ix_i\\
   b_k \leftarrow b_{k-1} + \eta y_i
   $$
   即$\hat w_k = \hat w_{k-1} + \eta y_i\hat x_i$，结合$\gamma =\mathop {\min}\limits_{i}\{y_i(w_{opt}\cdot x_i)+b_{opt}\}$，可得以下递推式：
   $$
   \begin{aligned}
   \hat w_k \cdot \hat w_{opt} &= (\hat w_{k-1} + \eta y_i\hat x_i)\cdot \hat w_{opt}\\
   &=\hat w_{k-1}\cdot \hat w_{opt}+\eta\cdot( y_i\hat w_{opt}  x_i)\\
   &\ge\hat w_{k-1} \cdot \hat w_{opt}+\eta \cdot\gamma\\
   &\ge\hat w_{k-2} \cdot \hat w_{opt}+2\times\eta \cdot\gamma\\
   &..\\
   &\ge k\times\eta \cdot\gamma
   \end{aligned}
   $$

   $$
   \begin{aligned}
   ||\hat w_k||^2 &=||\hat w_{k-1}+\eta y_i \hat x_i||^2\\
   &=||\hat w_{k-1}||^2+2\eta y_i\cdot \hat w_{k-1}+\eta^2||\hat x_i||^2\\
   &\le ||\hat w_{k-1}||^2 + \eta^2R^2\\
   &\le ||\hat w_{k-2}||^2 + 2 \times\eta^2R^2 \le \cdots\\
   &\le k \times \eta^2R^2
   
   \end{aligned}
   $$

   **综合上式可得**
   $$
   k\eta \gamma\le \hat w_k \cdot \hat w_{opt}\le ||\hat w_k|| \cdot ||\hat w_{opt}|\le \sqrt{k}\eta R\\
   k\eta\gamma \le \sqrt{k}\eta R\\
   k \le \left(\frac{R}{\gamma}\right)^2
   $$
   **该定理表明，误分类的次数$k$是有上界的**，*经过有限次搜索*可以找到**将训练数据完全正确分开的分离超平面**。通俗地说，当训练数据集是线性可分时，感知机学习算法原始形式地迭代是收敛的。

   ### 2.3.3 感知机学习算法的对偶形式

   对偶形式的基本思想：**将$w,b$表示为实例$x_i$和标记$y_i$的线性组合的形式**，通过求解其**系数**而得到$w$和$b$。在原始算法中，参数$w,b$是逐步修改的。假设一共修改$n$次，则$(w,b)$关于$(x_i, y_i)$的增量分别是$\alpha_iy_ix_i$和$\alpha_iy_i$，其中$\alpha_i=n_i\eta$，最后学习到的$w, b$分别表示如下：
   $$
   w = \sum_{i=1}^N \alpha_i\cdot y_i x_i\\
   b = \sum_{i=1}^N \alpha_i \cdot y_i
   $$
   当$\eta=1$时，表示第$i$个实例点由于误分而进行更新的次数。实例点更新次数越多，意味着距离分离超平面越近，也就越难正确分类。

   ### 算法 2.2 （对偶形式的感知机学习算法）

   **输入：**线性可分的数据集$T=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N,y_N)\}$，其中$x_i \in R^n,y_i\in {-1, +1}, i=1,2,3,\cdots,N$，学习率$\eta(0<\eta\le 1)$；

   **输出**：$\alpha, b$；**感知机模型**$f(x)=sign\left(\sum_{j=1}^N \alpha_j y_jx_j \cdot x + b\right)$，其中$\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_N)^T$。

   （1）$\alpha \leftarrow 0, b \leftarrow 0$；

   （2）在训练集中选取数据 $(x_i, y_i)$；

   （3）如果$y_i \left(\sum_{j=1}^N \alpha_jy_jx_j \cdot x_i + b\right) \le 0$：
   $$
   \alpha_i \leftarrow \alpha_i +\eta\\
   
   b \leftarrow b + \eta y_i 
   $$
   （4）转至（2）直到没有误分类数据。

   （5）计算并更新$w=\sum_{i=1}^N \alpha_iy_ix_i$

   *注*：对偶形式中训练实例**仅以内积**的形式出现，为了方便，可以预先将*训练集*中实例间的内积计算起来并以**矩阵**的形式存储，这个矩阵就是所谓的**Gram矩阵（Gram matrix）**。
   $$
   G=[x_i \cdot x_j]_{N\times N}
   $$
   
   
   
   
